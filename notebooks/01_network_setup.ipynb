{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#01 - Unpacking Neural Networks"
      ],
      "metadata": {
        "id": "aP5hMiUxVUYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin my investigation, I needed to deepen my understanding behind the mechanisms of deep learning. Combining online resources (eg. 3Blue1Brown's Neural Networks YouTube series) with calculus concepts learned in my math class, the code below establishes a simple binary classifier and explores the setup of a loss function."
      ],
      "metadata": {
        "id": "D2PB9eTPVZlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, pathlib\n",
        "sys.path.append(str(pathlib.Path.cwd().parent / \"src\"))\n",
        "from utils import make_mlp"
      ],
      "metadata": {
        "id": "mtEwPCYQWx3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing neural network modules from PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleNeuralNetwork(nn.Module):\n",
        "  #Initializes a standard neural network with one hidden layer\n",
        "  def __init__(self, activation = torch.relu):\n",
        "    #Hidden layer size -> tunable hyperparameter\n",
        "    hidden_size = 32\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(11, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    #One neuron in output layer -> binary classifier\n",
        "    self.output = nn.Linear(hidden_size, 1)\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward(self, x):\n",
        "    #Forward propagation - currently running on the ReLU activation function\n",
        "    x = self.activation(self.fc1(x))\n",
        "    x = self.activation(self.fc2(x))\n",
        "    x = self.output(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "model = SimpleNeuralNetwork()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhF8FZC_tYEu",
        "outputId": "6db5f596-f964-4f1d-d2ce-a61f4ebb20c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleNeuralNetwork(\n",
            "  (fc1): Linear(in_features=11, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#Using the Binary Cross-Entropy loss function, a Maximum Likelihood Estimator (MLE) for binary classification\n",
        "criterion = nn.BCELoss()\n",
        "#Adam optimizer yields general network performance gains\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
      ],
      "metadata": {
        "id": "0ydzC702wp8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}