{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#03 - Models for Analyzing Loss Landscapes"
      ],
      "metadata": {
        "id": "ECcMCFJnaS6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights on loss function smoothness and curvature strongly correlate with a network's performance and training stability. Hence, to deeply evaluate the impact of each activation function on network dynamics, I have used higher-order differential models like Hessian matrices and Lipschitz conditions to unpack loss surfaces. In real-life scenarios, however, full-application of these methods are computationally infeasible - tools from linear algebra like spectral decomposition and power iteration helped me accurately approximate these methods"
      ],
      "metadata": {
        "id": "a_IJ7aUpcypv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing module dependencies from previous notebooks\n",
        "import sys, pathlib\n",
        "sys.path.append(str(pathlib.Path.cwd().parent / \"src\"))\n",
        "from utils import make_mlp, set_seed, plot_loss"
      ],
      "metadata": {
        "id": "O8Wx18_laTP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating spectral norm of weight matrix\n",
        "def spectral_norm(layer):\n",
        "  weight = layer.weight.data\n",
        "  #Conducting singular value decomposition on weight matrices\n",
        "  U, S, V = torch.linalg.svd(weight, full_matrices=False)\n",
        "  lip = S.max()\n",
        "  return lip\n",
        "\n",
        "#Finding Lipschitz constant of model outputs w.r.t inputs\n",
        "def lipschitz_upper_bound(model, activation_bound):\n",
        "  upper_bound = 1.0\n",
        "  num_layers = len(list(model.modules()))\n",
        "  #Iterating over layers to find data points\n",
        "  for i, layer in enumerate(model.modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "      #Scaling spectral norm by activation function bounds\n",
        "      norm = spectral_norm(layer)\n",
        "      upper_bound *= norm\n",
        "      if i < num_layers - 1:\n",
        "        upper_bound *= activation_bound\n",
        "      else:\n",
        "        upper_bound *= 0.25\n",
        "  return upper_bound"
      ],
      "metadata": {
        "id": "bL6Lv_PfGRQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the Lipschitz constant w.r.t model gradients (Jacobian matrix)\n",
        "def lipschitz_lower_bound(model, x):\n",
        "  #Formatting inputs and outputs for gradient calculations\n",
        "  x = x.clone().detach().requires_grad_(True)\n",
        "  y = model(x)\n",
        "  y = y.squeeze(-1)\n",
        "  #Finding the final Jacobian matrix, differentiating loss w.r.t outputs\n",
        "  grad = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
        "  #Using the L2 norm operation to find Euclidean distance\n",
        "  grad_norm = torch.norm(grad, p=2, dim=-1)\n",
        "  return grad_norm.max().detach().numpy()\n"
      ],
      "metadata": {
        "id": "5rPHKKwi1oom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math, torch\n",
        "from torch.func import functional_call\n",
        "from torch.autograd.functional import hvp\n",
        "\n",
        "def max_hessian_eigval(model, criterion, x, y, iters=20, tol=1e-6):\n",
        "\n",
        "    #Creating a tensor of trainable parameters\n",
        "    p_items      = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
        "    names        = [n for n, _ in p_items]\n",
        "    params0      = tuple(p.detach().clone().requires_grad_(True) for _, p in p_items)\n",
        "    buffers_dict = dict(model.named_buffers())\n",
        "\n",
        "    #Formatting loss function to accept parameter tensor\n",
        "    def loss_fn(*params):\n",
        "        state = {**{k: v for k, v in zip(names, params)}, **buffers_dict}\n",
        "        out   = functional_call(model, state, (x,))\n",
        "        return criterion(out, y)\n",
        "\n",
        "    #Defining tensor operations to compute Hessian-vector products (HVPs)\n",
        "    def tup_dot(a, b):\n",
        "        return sum((ai.flatten() * bi.flatten()).sum() for ai, bi in zip(a, b))\n",
        "    def tup_norm(a):\n",
        "        return math.sqrt(tup_dot(a, a).item())\n",
        "    def tup_scale(a, s):\n",
        "        return tuple(ai / s for ai in a)\n",
        "\n",
        "    #Power iteration algorithm -> calculating repeated HVPs\n",
        "    v = tuple(torch.randn_like(p) for p in params0)\n",
        "    v = tup_scale(v, tup_norm(v))\n",
        "\n",
        "    for _ in range(iters):\n",
        "        _, Hv = hvp(loss_fn, params0, v)\n",
        "        nrm    = tup_norm(Hv)\n",
        "        if nrm < tol:\n",
        "            return 0.0\n",
        "        v = tup_scale(Hv, nrm)\n",
        "\n",
        "    #Calculating the Rayleigh Quotient to isolate largest eigenvalue\n",
        "    _, Hv = hvp(loss_fn, params0, v)\n",
        "    return tup_dot(v, Hv).item()"
      ],
      "metadata": {
        "id": "mGhqp9J3aVs5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}