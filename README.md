# neural-network-sigmoid-vs-relu
Training a vanilla neural network to perform binary classification using both Sigmoid and ReLU as activation functions. Analyzing the impact of activation function on loss surface curvature using higher-order differential models like Hessians and Lipschitz Conditions, approximated with techniques from linear algebra like power iteration. 
